{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Lab: Logistic Regression and SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names:\n",
    "Dylan Scott\n",
    "Jobin Joseph\n",
    "Nnenna Okpara\n",
    "Satvik Ajmera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "You are to perform predictive analysis (classification) upon a data set: model the dataset using\n",
    "methods we have discussed in class: logistic regression and support vector machines, and making\n",
    "conclusions from the analysis. Follow the CRISP-DM framework in your analysis (you are not\n",
    "performing all of the CRISP-DM outline, only the portions relevant to the grading rubric outlined\n",
    "below). This report is worth 10% of the final grade. You may complete this assignment in teams of\n",
    "as many as three people.\n",
    "\n",
    "Write a report covering all the steps of the project. The format of the document can be PDF,\n",
    "*.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in the\n",
    "rendered iPython notebook. The results should be reproducible using your report. Please carefully\n",
    "describe every assumption and every step in your report.\n",
    "\n",
    "SVM and Logistic Regression Modeling\n",
    "• [50 points] Create a logistic regression model and a support vector machine model for the\n",
    "classification task involved with your dataset. Assess how well each model performs (use\n",
    "80/20 training/testing split for your data). Adjust parameters of the models to make them more\n",
    "accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel\n",
    "only is fine to use.\n",
    "\n",
    "[pick performance stats]\n",
    "\n",
    "• [10 points] Discuss the advantages of each model for each classification task. Does one type\n",
    "of model offer superior performance over another in terms of prediction accuracy? In terms of\n",
    "training time or efficiency? Explain in detail.\n",
    "\n",
    "• [30 points] Use the weights from logistic regression to interpret the importance of different\n",
    "features for each classification task. Explain your interpretation in detail. Why do you think\n",
    "some variables are more important?\n",
    "\n",
    "• [10 points] Look at the chosen support vectors for the classification task. Do these provide\n",
    "any insight into the data? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset add-on\n",
    "From the first project we submitted we have since added on more data that we found on the NTSB website. We were able to merge in new columns using join as well as apend on more recent data. This will give us more vairables but we will have to clean up some of those added rows. This next section will be the clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read in the Aviation Data\n",
    "final_data = pd.read_csv(\"Data/final_data.csv\",low_memory=False,dtype={'damage': str})\n",
    "#Delete columns that were imported incorrectly\n",
    "del final_data[\"Unnamed: 0\"]\n",
    "del final_data[\"dprt_state.1\"]\n",
    "del final_data[\"index\"]\n",
    "del final_data[\"ntsb_no_x\"]\n",
    "final_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It looks like we have some missing values and have an inconsistant UNK vs UNK on flight damage\n",
    "#combine all injuries includigng those on the ground\n",
    "#sky_cond_ceil, sky_cond_nonceil\n",
    "#chekc U vs Unk for wind_vel_ind\n",
    "#flight crew \n",
    "finaldamagecount = final_data[\"damage\"].value_counts().reset_index()\n",
    "finaldamagecount.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like we have some inconsistant cities since some are upper and some are lower case\n",
    "final_data['ev_city'] = final_data['ev_city'].str.upper()\n",
    "ev_city_fix = final_data[\"ev_city\"].value_counts().reset_index()\n",
    "ev_city_fix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looks like we have some inconsistant cities since some are upper and some are lower case\n",
    "final_data['ev_city'] = final_data['ev_city'].str.upper()\n",
    "ev_city_fix = final_data[\"ev_city\"].value_counts().reset_index()\n",
    "ev_city_fix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.loc[final_data['damage'].str.contains('UNK', na=False), 'damage'] = 'UNK'\n",
    "finaldamagecount = final_data[\"damage\"].value_counts().reset_index()\n",
    "finaldamagecount.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see if wind_vel_ind had a miss-match with U and UNK\n",
    "wind_count = final_data[\"wind_vel_ind\"].value_counts().reset_index()\n",
    "wind_count.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with unknnowns\n",
    "#some columns we can't simply replace the blank value with \"Unknown\" or 0s since that will skew our data\n",
    "#'cert_max_gr_wt','afm_hrs_last_insp','rwy_len','rwy_width'\n",
    "# with the columns listed above we have elected to remove any rows where they are blank. This will help focus our data and it will still leave us with an ample amount of data\n",
    "final_data.dropna(subset=['cert_max_gr_wt','afm_hrs_last_insp','rwy_len','rwy_width'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the injuries columns to make them easier to read\n",
    "final_data = final_data.rename(columns={\"inj_tot_f\": \"Total_Fatal_Injuries\", \"inj_tot_s\": \"Total_Serious_Injuries\",\"inj_tot_m\":\"Total_Minor_Injuries\",\"inj_tot_n\":'Total_Uninjured',\"inj_tot_t\":\"Total_Injuries_Flight\"})\n",
    "\n",
    "#fill in 0s when there wasn't an injury in that category\n",
    "final_data.update(final_data[['Total_Fatal_Injuries','Total_Serious_Injuries','Total_Minor_Injuries','Total_Uninjured','Total_Injuries_Flight','inj_f_grnd','inj_m_grnd','inj_s_grnd']].fillna(0))\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set missing variables to Unknown in order to run our models\n",
    "final_data.update(final_data.fillna(\"UNK\"))\n",
    "final_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using code from this classes Github: \n",
    "https://github.com/jakemdrew/DataMiningNotebooks/blob/master/04.%20Logits%20and%20SVM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to account for ALL injuries. This includes injuries on the ground as well as passangers\n",
    "#Here we will make a new column that shows total injuries including ground ones\n",
    "final_data['Total_Injuries_Ground'] = final_data['inj_f_grnd']+final_data['inj_m_grnd']+final_data['inj_s_grnd']\n",
    "final_data['Total_Injuries'] = final_data['Total_Injuries_Ground']+final_data['Total_Injuries_Flight']\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column of injuried or not to get a binary response\n",
    "#1 means someone was hurt 0 means someone was not\n",
    "final_data['Injury'] = np.where(final_data['Total_Injuries'] >0,1,0)\n",
    "injuries = final_data[\"Injury\"].value_counts().reset_index()\n",
    "injuries.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the index column called \"Unnamed: 0\"\n",
    "final_df = final_data.copy()\n",
    "#Since we added up all of our injuries we don't need the other columns that include injury count since it will be colinear to our prediction variable\n",
    "final_df = final_df.drop(['Total_Fatal_Injuries','Total_Serious_Injuries','Total_Minor_Injuries','Total_Uninjured','Total_Injuries_Flight','inj_f_grnd','inj_m_grnd','inj_s_grnd','Total_Injuries_Ground',\"Total_Injuries\",\"ev_id\", \"dprt_city\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(\"Injury\", axis = 1).copy()\n",
    "y = final_df[\"Injury\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not run this cell!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# # Define which columns should be encoded vs scaled\n",
    "# #Categorical columns to convert to one hot encoding\n",
    "# columns_to_encode = ['acft_make',\"acft_model\",\"acft_category\", \"damage\",\n",
    "#                      \"far_part\",\"type_fly\",\"dprt_state\",\n",
    "#                      \"ev_type\",\"ev_city\",\"ev_state\",\"ev_country\",\n",
    "#                      \"ev_highest_injury\",\"sky_cond_ceil\",\n",
    "#                      \"sky_cond_nonceil\",\"wind_vel_ind\",\"wx_int_precip\",\n",
    "#                      \"phase_flt_spec\"]\n",
    "# #Continuous columns to be scaled\n",
    "# columns_to_scale  = ['cert_max_gr_wt', 'afm_hrs_last_insp',\n",
    "#                      'rwy_len',\"rwy_width\"]\n",
    "\n",
    "# # Instantiate encoder/scaler\n",
    "# scaler = StandardScaler()\n",
    "# ohe = OneHotEncoder(drop=\"first\")\n",
    "\n",
    "# # Scale and Encode Separate Columns\n",
    "# scaled_columns = scaler.fit(X[columns_to_scale]) \n",
    "# a = scaled_columns.transform(X)\n",
    "# encoded_columns = ohe.fit(X[columns_to_encode])\n",
    "# b = encoded_columns.transform(X)\n",
    "# # Concatenate (Column-Bind) Processed Columns Back Together\n",
    "# c = np.concatenate([a, b], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop=\"first\")\n",
    "encoder.fit(X)\n",
    "X = encoder.transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl_obj = StandardScaler(with_mean=False)\n",
    "scl_obj.fit(X_train) \n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) \n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(class_weight=\"balanced\")\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "y_hat = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print(acc)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we looked to see if there were any variables that may be too coorlated with our regression. We got a 99% prediction rate which told us we had a variable that should be considered \"unknown\" We removed ev_highest_injury which should be treated as unknown. This fixes our issues of too high of accurcy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models: USE THIS HOMIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df.copy()\n",
    "del df['ev_highest_injury']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Injury\", axis = 1).copy()\n",
    "y = df[\"Injury\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding with proper model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop=\"first\")\n",
    "encoder.fit(X)\n",
    "X = encoder.transform(X)\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler(with_mean=False)\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(class_weight=\"balanced\",solver='liblinear',penalty = 'l2')\n",
    "logisticRegr.fit(X_train_scaled, y_train)\n",
    "y_hat = logisticRegr.predict(X_test_scaled)\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print(acc)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT Scaled\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(class_weight=\"balanced\",solver='lbfgs', max_iter=1000)\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "y_hat = logisticRegr.predict(X_test)\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print(acc)\n",
    "print(conf)\n",
    "### timer added in the script to see the efficency of the model \n",
    "import timeit\n",
    "\n",
    "def test(n):\n",
    "    return sum(range(n))\n",
    "\n",
    "n = 10000\n",
    "loop = 1000\n",
    "\n",
    "result = timeit.timeit('test(n)', globals=globals(), number=loop)\n",
    "print(result / loop)\n",
    "# 0.0002666301020071842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Injury' in final_df:\n",
    "    y = final_df['Injury'].values # get the labels we want\n",
    "    del final_df['Injury'] # get rid of the class label\n",
    "    X = final_df.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(logisticRegr.coef_.T,final_df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = logisticRegr.coef_.T # take transpose to make a column vector\n",
    "variable_names = final_df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(logisticRegr.coef_[0],index=final_df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "### timer added in the script to see the efficency of the model \n",
    "import timeit\n",
    "\n",
    "def test(n):\n",
    "    return sum(range(n))\n",
    "\n",
    "n = 10000\n",
    "loop = 1000\n",
    "\n",
    "result = timeit.timeit('test(n)', globals=globals(), number=loop)\n",
    "print(result / loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#took forever to run\n",
    "\n",
    "# train the model just as before\n",
    "#svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)\n",
    "\n",
    "### timer added in the script to see the efficency of the model \n",
    "import timeit\n",
    "\n",
    "def test(n):\n",
    "    return sum(range(n))\n",
    "\n",
    "n = 10000\n",
    "loop = 1000\n",
    "\n",
    "result = timeit.timeit('test(n)', globals=globals(), number=loop)\n",
    "print(result / loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "one hot encoding\n",
    "avoid confounding variables - this causes an issue with feature importance\n",
    "check for highly corrilated variables - \n",
    "use a confusion matrix\n",
    "scale our data\n",
    "large diff in KDE for support vectors - it falls along the decision bountry vs the read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
